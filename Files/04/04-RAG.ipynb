{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate a RAG system\n",
    "\n",
    "In this notebook, you'll ingest and preprocess data, create embeddings, and build a FAISS index, ultimately enabling you to implement a RAG system effectively.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "Install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain openai faiss-cpu transformers nltk pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to define the values that will be used when submitting a chat completion request through the API endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL for your Azure OpenAI Service endpoint\n",
    "# Replace 'Your Azure OpenAI Service Endpoint' with your actual endpoint URL obtained previously\n",
    "api_base = 'Your Azure OpenAI Service Endpoint'\n",
    "\n",
    "# Define the API key for your Azure OpenAI Service\n",
    "# Replace 'Your Azure OpenAI Service API Key' with your actual API key obtained previously\n",
    "api_key = 'Your Azure OpenAI Service API Key'\n",
    "\n",
    "# Define the names of the models deployed in your Azure OpenAI Service\n",
    "model_name = 'gpt-4'\n",
    "\n",
    "# Define the API version to use for the Azure OpenAI Service\n",
    "api_version = '2024-08-01-preview'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to load your dataset into the notebook and preprocess it. Then, create embeddings for each document and build a FAISS index for efficient similarity search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import nltk\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('app_hotel_reviews.csv')\n",
    "documents = data['User Reviews'].tolist()\n",
    "\n",
    "# Preprocess the text\n",
    "nltk.download('stopwords')\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "cleaned_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=\"text-embedding-ada-002\",\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    azure_endpoint=api_base,\n",
    "    openai_api_key=api_key,\n",
    "    chunk_size=1\n",
    ")\n",
    "document_embeddings = [embeddings.embed(doc) for doc in cleaned_documents]\n",
    "\n",
    "# Build the FAISS index\n",
    "faiss_index = FAISS(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll create an instance of the AzureOpenAI client to interact with your Azure OpenAI Service and obtain the chat completion response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AzureOpenAI class from the openai library\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Create two instances of the AzureOpenAI client to interact with Azure's OpenAI Service\n",
    "client = AzureOpenAI(\n",
    "    # Use the API key for authentication\n",
    "    api_key=api_key,  \n",
    "    \n",
    "    # Specify the API version to use\n",
    "    api_version=api_version,\n",
    "    \n",
    "    # Construct the base URL for the deployment using the provided API base and deployment name\n",
    "    base_url=f\"{api_base}openai/deployments/{model_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the messages to send to the models\n",
    "messages=[\n",
    "    { \n",
    "        \"role\": \"user\", \n",
    "        \"content\": [  \n",
    "            { \n",
    "                # Specify the type of content as text\n",
    "                \"type\": \"text\", \n",
    "                    \n",
    "                # Provide the text content for the model to process\n",
    "                \"text\": \"Where can I stay in London?\" \n",
    "            }\n",
    "        ] \n",
    "    } \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the pipeline to retrieve relevant information from FAISS and generate responses with Azure OpenAIâ€™s GPT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalAugmentedGenerationChain\n",
    "\n",
    "# Define the RAG pipeline\n",
    "rag_chain = RetrievalAugmentedGenerationChain(\n",
    "    retriever=faiss_index,\n",
    "    generator=client.chat.completions.create,\n",
    "    prompt_template=\"Retrieve and generate response for: {query}\"\n",
    "    )\n",
    "\n",
    "# Example query\n",
    "query = \"Where can I stay in London?\"\n",
    "response = rag_chain.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chat completion requests using the AzureOpenAI clients\n",
    "response1 = client1.chat.completions.create(\n",
    "    # Specify the model to use for generating the response\n",
    "    model=model_name1,\n",
    "    \n",
    "    # Define the messages to send to the model\n",
    "    messages=messages1,\n",
    "    \n",
    "    # Set the maximum number of tokens to generate in the response\n",
    "    max_tokens=2000 \n",
    ")\n",
    "\n",
    "response2 = client2.chat.completions.create(\n",
    "    model=model_name2,\n",
    "    messages=messages2,\n",
    "    max_tokens=2000 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The response contains multiple choices, and we are accessing the first one as our result\n",
    "result1 = response1.choices[0].message.content\n",
    "result2 = response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables `result1` and `result2` now contain the content of the first choice from their respective responses. This content is the generated text or code from the model based on the input messages. You can print each result, copy the code block generated within them, run each of the codes in a new code cell and compare their outputs. Are the scripts and outputs in any way different? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit more requests and have the code modified. It will also further demonstrate the difference between the models and make the metrics observed later on more significant. However, to make sure that the models keep track of the prompt history, we need to append their responses and the new prompts to the `messages` variables that we've been using so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the responses to the messages as an Assistant Role\n",
    "messages1.append({\"role\": \"assistant\", \"content\": result1})\n",
    "messages2.append({\"role\": \"assistant\", \"content\": result2})\n",
    "\n",
    "# Define the new prompt that will develop the chat completion further\n",
    "new_prompt = \"Add a legend to the plot replacing the labels\"\n",
    "\n",
    "# Add the user's question to the messages as a User Role\n",
    "messages1.append({\"role\": \"user\", \"content\": new_prompt})\n",
    "messages2.append({\"role\": \"user\", \"content\": new_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the new chat completion requests\n",
    "response1 = client1.chat.completions.create(\n",
    "    model=model_name1,\n",
    "    messages=messages1,\n",
    "    max_tokens=2000 \n",
    ")\n",
    "response2 = client2.chat.completions.create(\n",
    "    model=model_name2,\n",
    "    messages=messages2,\n",
    "    max_tokens=2000 \n",
    ")\n",
    "result1 = response1.choices[0].message.content\n",
    "result2 = response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After reviewing the plot and remembering the benchmark values in the Accuracy vs. Cost chart observed before, can you conclude which model is best for your use case? Does the difference in the outputs' accuracy outweight the difference in tokens generated and therefore cost? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "If you've finished the exercise, you should delete the resources you have created to avoid incurring unnecessary Azure costs.\n",
    "\n",
    "1. Return to the browser tab containing the Azure portal (or re-open the [Azure portal](https://portal.azure.com?azure-portal=true) in a new browser tab) and view the contents of the resource group where you deployed the resources used in this exercise.\n",
    "1. On the toolbar, select **Delete resource group**.\n",
    "1. Enter the resource group name and confirm that you want to delete it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
